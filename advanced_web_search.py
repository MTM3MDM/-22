# ê³ ê¸‰ ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ
# ì‹¤ì‹œê°„ ê²€ìƒ‰, ë‹¤ì¤‘ ì†ŒìŠ¤, ê²°ê³¼ ìš”ì•½ ë° ë¶„ì„

import asyncio
import aiohttp
import json
import logging
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import quote, urljoin
import hashlib
from bs4 import BeautifulSoup, Tag
import requests
from dataclasses import dataclass
from types import TracebackType

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    url: str
    snippet: str
    source: str
    timestamp: str
    relevance_score: float = 0.0

class AdvancedWebSearcher:
    """ê³ ê¸‰ ì›¹ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self):
        self.search_engines = {
            'google': self._search_google,
            'bing': self._search_bing,
            'duckduckgo': self._search_duckduckgo,
            'wikipedia': self._search_wikipedia,
            'reddit': self._search_reddit,
            'github': self._search_github,
            'stackoverflow': self._search_stackoverflow
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        self.session = None
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers=self.headers,
            connector=aiohttp.TCPConnector(limit=100, limit_per_host=10)
        )
        return self
    
    async def __aexit__(self, 
                      exc_type: Optional[type[BaseException]], 
                      exc_val: Optional[BaseException], 
                      exc_tb: Optional[TracebackType]):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    def _get_cache_key(self, query: str, engine: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(f"{query}_{engine}".encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        return time.time() - cache_entry.get('timestamp', 0) < self.cache_ttl
    
    async def _search_google(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """Google ê²€ìƒ‰ (ìŠ¤í¬ë˜í•‘ ë°©ì‹)"""
        if not self.session:
            return []
        try:
            search_url = f"https://www.google.com/search?q={quote(query)}&num={max_results}&hl=ko"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    logger.warning(f"Google ê²€ìƒ‰ ì‹¤íŒ¨: {response.status}")
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                results: List[SearchResult] = []
                search_results = soup.find_all('div', class_='g')
                
                for result in search_results[:max_results]:
                    if not isinstance(result, Tag):
                        continue
                    try:
                        title_elem = result.find('h3')
                        link_elem = result.find('a')
                        
                        # ìŠ¤ë‹ˆí«ì„ ì°¾ê¸° ìœ„í•œ ì—¬ëŸ¬ í´ë˜ìŠ¤ ì‹œë„
                        snippet_elem = result.find('div', attrs={"data-sncf": "1"})
                        if not snippet_elem:
                            snippet_elem = result.find('div', class_='VwiC3b')

                        if title_elem and isinstance(title_elem, Tag) and link_elem and isinstance(link_elem, Tag):
                            title = title_elem.get_text(strip=True)
                            url_val = link_elem.get('href')
                            url = str(url_val) if url_val else ""
                            
                            snippet = ""
                            if snippet_elem and isinstance(snippet_elem, Tag):
                                snippet = snippet_elem.get_text(strip=True)
                            
                            if url.startswith('/url?q='):
                                url = url.split('/url?q=')[1].split('&')[0]
                            
                            if url: # URLì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                                results.append(SearchResult(
                                    title=title,
                                    url=url,
                                    snippet=snippet,
                                    source="Google",
                                    timestamp=datetime.now().isoformat()
                                ))
                    except Exception as e:
                        logger.debug(f"Google ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                logger.info(f"Googleì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                return results
                
        except Exception as e:
            logger.error(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_bing(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """Bing ê²€ìƒ‰"""
        if not self.session:
            return []
        try:
            search_url = f"https://www.bing.com/search?q={quote(query)}&count={max_results}"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                results: List[SearchResult] = []
                search_results = soup.find_all('li', class_='b_algo')
                
                for result in search_results[:max_results]:
                    if not isinstance(result, Tag):
                        continue
                    try:
                        title_elem = result.find('h2')
                        snippet_elem = result.find('p')
                        
                        if title_elem and isinstance(title_elem, Tag):
                            link_elem = title_elem.find('a')
                            if link_elem and isinstance(link_elem, Tag):
                                title = title_elem.get_text(strip=True)
                                url_val = link_elem.get('href')
                                url = str(url_val) if url_val else ""
                                
                                snippet = ""
                                if snippet_elem and isinstance(snippet_elem, Tag):
                                    snippet = snippet_elem.get_text(strip=True)
                                
                                if url: # URLì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                                    results.append(SearchResult(
                                        title=title,
                                        url=url,
                                        snippet=snippet,
                                        source="Bing",
                                        timestamp=datetime.now().isoformat()
                                    ))
                    except Exception as e:
                        continue
                
                logger.info(f"Bingì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                return results
                
        except Exception as e:
            logger.error(f"Bing ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_duckduckgo(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """DuckDuckGo ê²€ìƒ‰"""
        if not self.session:
            return []
        try:
            # DuckDuckGo Instant Answer API ì‚¬ìš©
            search_url = f"https://api.duckduckgo.com/?q={quote(query)}&format=json&no_html=1&skip_disambig=1"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    return []
                
                data = await response.json()
                results: List[SearchResult] = []
                
                # Abstract ê²°ê³¼
                if data.get('Abstract'):
                    results.append(SearchResult(
                        title=data.get('Heading', query),
                        url=data.get('AbstractURL', ''),
                        snippet=data.get('Abstract', ''),
                        source="DuckDuckGo",
                        timestamp=datetime.now().isoformat()
                    ))
                
                # Related Topics
                for topic in data.get('RelatedTopics', [])[:max_results-1]:
                    if isinstance(topic, dict) and 'Text' in topic and 'FirstURL' in topic:
                        results.append(SearchResult(
                            title=str(topic.get('Text', ''))[:100] + "...",
                            url=str(topic.get('FirstURL', '')),
                            snippet=str(topic.get('Text', '')),
                            source="DuckDuckGo",
                            timestamp=datetime.now().isoformat()
                        ))
                
                logger.info(f"DuckDuckGoì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                return results
                
        except Exception as e:
            logger.error(f"DuckDuckGo ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_wikipedia(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Wikipedia ê²€ìƒ‰"""
        if not self.session:
            return []
        try:
            # Wikipedia API ì‚¬ìš©
            search_url = f"https://ko.wikipedia.org/api/rest_v1/page/summary/{quote(query)}"
            
            async with self.session.get(search_url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    result = SearchResult(
                        title=data.get('title', ''),
                        url=data.get('content_urls', {}).get('desktop', {}).get('page', ''),
                        snippet=data.get('extract', ''),
                        source="Wikipedia",
                        timestamp=datetime.now().isoformat()
                    )
                    
                    logger.info("Wikipediaì—ì„œ 1ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                    return [result]
                
                # ê²€ìƒ‰ API ì‚¬ìš©
                search_url = f"https://ko.wikipedia.org/w/api.php?action=query&list=search&srsearch={quote(query)}&format=json&srlimit={max_results}"
                
                async with self.session.get(search_url) as response:
                    if response.status != 200:
                        return []
                    
                    data = await response.json()
                    results: List[SearchResult] = []
                    
                    for item in data.get('query', {}).get('search', []):
                        if isinstance(item, dict):
                            results.append(SearchResult(
                                title=item.get('title', ''),
                                url=f"https://ko.wikipedia.org/wiki/{quote(item.get('title', ''))}",
                                snippet=re.sub(r'<[^>]+>', '', item.get('snippet', '')),
                                source="Wikipedia",
                                timestamp=datetime.now().isoformat()
                            ))
                    
                    logger.info(f"Wikipediaì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                    return results
                
        except Exception as e:
            logger.error(f"Wikipedia ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_reddit(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Reddit ê²€ìƒ‰"""
        if not self.session:
            return []
        try:
            search_url = f"https://www.reddit.com/search.json?q={quote(query)}&limit={max_results}&sort=relevance"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    return []
                
                data = await response.json()
                results: List[SearchResult] = []
                
                for post in data.get('data', {}).get('children', []):
                    if isinstance(post, dict):
                        post_data = post.get('data', {})
                        if isinstance(post_data, dict):
                            results.append(SearchResult(
                                title=post_data.get('title', ''),
                                url=f"https://reddit.com{post_data.get('permalink', '')}",
                                snippet=post_data.get('selftext', '')[:200] + "..." if post_data.get('selftext') else "",
                                source=f"Reddit (r/{post_data.get('subreddit', '')})",
                                timestamp=datetime.now().isoformat()
                            ))
                
                logger.info(f"Redditì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                return results
                
        except Exception as e:
            logger.error(f"Reddit ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_github(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """GitHub ê²€ìƒ‰"""
        if not self.session:
            return []
        try:
            search_url = f"https://api.github.com/search/repositories?q={quote(query)}&sort=stars&order=desc&per_page={max_results}"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    return []
                
                data = await response.json()
                results: List[SearchResult] = []
                
                for repo in data.get('items', []):
                    if isinstance(repo, dict):
                        results.append(SearchResult(
                            title=repo.get('full_name', ''),
                            url=repo.get('html_url', ''),
                            snippet=repo.get('description', '') or "ì„¤ëª… ì—†ìŒ",
                            source=f"GitHub (â­{repo.get('stargazers_count', 0)})",
                            timestamp=datetime.now().isoformat()
                        ))
                
                logger.info(f"GitHubì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                return results
                
        except Exception as e:
            logger.error(f"GitHub ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def _search_stackoverflow(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Stack Overflow ê²€ìƒ‰"""
        if not self.session:
            return []
        try:
            search_url = f"https://api.stackexchange.com/2.3/search/advanced?order=desc&sort=relevance&q={quote(query)}&site=stackoverflow&pagesize={max_results}"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    return []
                
                data = await response.json()
                results: List[SearchResult] = []
                
                for question in data.get('items', []):
                    if isinstance(question, dict):
                        results.append(SearchResult(
                            title=question.get('title', ''),
                            url=question.get('link', ''),
                            snippet=f"ì¡°íšŒìˆ˜: {question.get('view_count', 0)}, ë‹µë³€: {question.get('answer_count', 0)}",
                            source="Stack Overflow",
                            timestamp=datetime.now().isoformat()
                        ))
                
                logger.info(f"Stack Overflowì—ì„œ {len(results)}ê°œ ê²°ê³¼ ìˆ˜ì§‘")
                return results
                
        except Exception as e:
            logger.error(f"Stack Overflow ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def search_multiple_engines(self, query: str, engines: Optional[List[str]] = None, max_results_per_engine: int = 5) -> Dict[str, List[SearchResult]]:
        """ë‹¤ì¤‘ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê²€ìƒ‰"""
        if engines is None:
            engines = ['google', 'wikipedia', 'github']
        
        results: Dict[str, List[SearchResult]] = {}
        tasks = []
        
        for engine in engines:
            if engine in self.search_engines:
                # ìºì‹œ í™•ì¸
                cache_key = self._get_cache_key(query, engine)
                if cache_key in self.cache and self._is_cache_valid(self.cache[cache_key]):
                    results[engine] = self.cache[cache_key]['results']
                    continue
                
                # ë¹„ë™ê¸° ê²€ìƒ‰ ì‘ì—… ì¶”ê°€
                task = asyncio.create_task(self.search_engines[engine](query, max_results_per_engine))
                tasks.append((engine, task))
        
        # ëª¨ë“  ê²€ìƒ‰ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        for engine, task in tasks:
            try:
                search_results = await task
                results[engine] = search_results
                
                # ìºì‹œì— ì €ì¥
                cache_key = self._get_cache_key(query, engine)
                self.cache[cache_key] = {
                    'results': search_results,
                    'timestamp': time.time()
                }
                
            except Exception as e:
                logger.error(f"{engine} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                results[engine] = []
        
        return results
    
    def _calculate_relevance_score(self, result: SearchResult, query: str) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        query_lower = query.lower()
        
        # ì œëª©ì—ì„œ ì¿¼ë¦¬ ë‹¨ì–´ ë§¤ì¹­
        title_lower = result.title.lower()
        query_words = query_lower.split()
        
        for word in query_words:
            if word in title_lower:
                score += 2.0
        
        # ìŠ¤ë‹ˆí«ì—ì„œ ì¿¼ë¦¬ ë‹¨ì–´ ë§¤ì¹­
        snippet_lower = result.snippet.lower()
        for word in query_words:
            if word in snippet_lower:
                score += 1.0
        
        # ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜
        source_weights = {
            'Wikipedia': 1.5,
            'GitHub': 1.3,
            'Stack Overflow': 1.2,
            'Google': 1.0,
            'Bing': 0.9,
            'DuckDuckGo': 0.8,
            'Reddit': 0.7
        }
        
        for source, weight in source_weights.items():
            if source in result.source:
                score *= weight
                break
        
        return score
    
    def merge_and_rank_results(self, search_results: Dict[str, List[SearchResult]], query: str, max_results: int = 10) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë° ìˆœìœ„ ë§¤ê¸°ê¸°"""
        all_results: List[SearchResult] = []
        seen_urls = set()
        
        # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°
        for engine, results in search_results.items():
            for result in results:
                if result.url and result.url not in seen_urls:
                    result.relevance_score = self._calculate_relevance_score(result, query)
                    all_results.append(result)
                    seen_urls.add(result.url)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return all_results[:max_results]
    
    async def comprehensive_search(self, query: str, engines: Optional[List[str]] = None, max_results: int = 10) -> Tuple[List[SearchResult], Dict[str, Any]]:
        """ì¢…í•© ê²€ìƒ‰ ìˆ˜í–‰"""
        start_time = time.time()
        
        if engines is None:
            # ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ì—”ì§„ ì„ íƒ
            if any(keyword in query.lower() for keyword in ['github', 'code', 'ì½”ë“œ', 'í”„ë¡œê·¸ë˜ë°']):
                engines = ['google', 'github', 'stackoverflow']
            elif any(keyword in query.lower() for keyword in ['reddit', 'ì»¤ë®¤ë‹ˆí‹°', 'í† ë¡ ']):
                engines = ['google', 'reddit', 'wikipedia']
            else:
                engines = ['google', 'wikipedia', 'bing']
        
        # ë‹¤ì¤‘ ì—”ì§„ ê²€ìƒ‰
        search_results = await self.search_multiple_engines(query, engines, max_results_per_engine=5)
        
        # ê²°ê³¼ ë³‘í•© ë° ìˆœìœ„ ë§¤ê¸°ê¸°
        final_results = self.merge_and_rank_results(search_results, query, max_results)
        
        # ê²€ìƒ‰ í†µê³„
        search_stats: Dict[str, Any] = {
            'query': query,
            'engines_used': list(search_results.keys()),
            'total_results_found': sum(len(results) for results in search_results.values()),
            'final_results_count': len(final_results),
            'search_time': time.time() - start_time,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"ì¢…í•© ê²€ìƒ‰ ì™„ë£Œ: '{query}' - {len(final_results)}ê°œ ê²°ê³¼, {search_stats['search_time']:.2f}ì´ˆ")
        
        return final_results, search_stats

class SearchResultFormatter:
    """ê²€ìƒ‰ ê²°ê³¼ í¬ë§·í„°"""
    
    @staticmethod
    def format_for_discord(results: List[SearchResult], stats: Dict[str, Any], max_length: int = 2000) -> str:
        """ë””ìŠ¤ì½”ë“œìš© ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…"""
        if not results:
            return "ğŸ” ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        query = stats.get('query', '')
        search_time = stats.get('search_time', 0)
        total_found = stats.get('total_results_found', 0)
        engines = ', '.join(stats.get('engines_used', []))
        
        # í—¤ë”
        header = f"ğŸ” **'{query}' ê²€ìƒ‰ ê²°ê³¼**\n"
        header += f"ğŸ“Š {total_found}ê°œ ê²°ê³¼ ë°œê²¬, {len(results)}ê°œ ì„ ë³„ ({search_time:.1f}ì´ˆ)\n"
        header += f"ğŸŒ ê²€ìƒ‰ ì—”ì§„: {engines}\n\n"
        
        # ê²°ê³¼ ëª©ë¡
        results_text = ""
        current_length = len(header)
        
        for i, result in enumerate(results, 1):
            # ì œëª© ê¸¸ì´ ì œí•œ
            title = result.title[:80] + "..." if len(result.title) > 80 else result.title
            
            # ìŠ¤ë‹ˆí« ê¸¸ì´ ì œí•œ
            snippet = result.snippet[:150] + "..." if len(result.snippet) > 150 else result.snippet
            
            result_str = f"**{i}. [{title}]({result.url})**\n"
            if result.source:
                result_str += f"   - ì¶œì²˜: `{result.source}`\n"
            if result.snippet:
                result_str += f"   - ìš”ì•½: *{snippet}*\n\n"
            else:
                result_str += "\n"

            if current_length + len(result_str) > max_length:
                break
            
            results_text += result_str
            current_length += len(result_str)
        
        return header + results_text

# --- ëª¨ë“ˆ API ---
# ì´ ì‹œìŠ¤í…œì˜ ê³µê°œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œëŠ” ì´ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

_searcher_instance: Optional[AdvancedWebSearcher] = None

async def initialize_web_search():
    """
    ê³ ê¸‰ ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    global _searcher_instance
    if _searcher_instance is None:
        _searcher_instance = AdvancedWebSearcher()
    logger.info("ê³ ê¸‰ ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ.")

async def search_web(query: str, engines: Optional[List[str]] = None, max_results: int = 5) -> str:
    """
    ì›¹ì„ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ Discordìš©ìœ¼ë¡œ í¬ë§·íŒ…í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if _searcher_instance is None:
        await initialize_web_search()
    
    # _searcher_instanceê°€ Noneì´ ì•„ë‹˜ì„ ë³´ì¥
    if _searcher_instance is None:
        logger.error("ì›¹ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return "ì˜¤ë¥˜: ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    async with _searcher_instance as searcher:
        results, stats = await searcher.comprehensive_search(query, engines, max_results)
        return SearchResultFormatter.format_for_discord(results, stats)

async def search_web_summary(query: str, engines: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    ì›¹ì„ ê²€ìƒ‰í•˜ê³ , ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if _searcher_instance is None:
        await initialize_web_search()

    # _searcher_instanceê°€ Noneì´ ì•„ë‹˜ì„ ë³´ì¥
    if _searcher_instance is None:
        logger.error("ì›¹ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return {
            "summary": "ì˜¤ë¥˜: ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "top_result": None,
            "stats": {}
        }

    async with _searcher_instance as searcher:
        results, stats = await searcher.comprehensive_search(query, engines, max_results=3)
        
        if not results:
            return {
                "summary": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "top_result": None,
                "stats": stats
            }
        
        top_result = results[0]
        
        # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì—¬ê¸°ì„œëŠ” ìŠ¤ë‹ˆí«ì„ ì‚¬ìš©í•˜ì§€ë§Œ, LLMì„ ì´ìš©í•´ ë” ì •êµí•œ ìš”ì•½ ê°€ëŠ¥)
        summary = f"ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ ê²°ê³¼ëŠ” '{top_result.source}'ì—ì„œ ì°¾ì€ '{top_result.title}'ì…ë‹ˆë‹¤. "
        summary += f"ìš”ì•½ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {top_result.snippet}"
        
        return {
            "summary": summary,
            "top_result": top_result,
            "stats": stats
        }