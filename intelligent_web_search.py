"""
ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ë° ê¸°ì–µ ì‹œìŠ¤í…œ
SerpAPIë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ê²€ìƒ‰ê³¼ ë²¡í„° ê¸°ë°˜ ê¸°ì–µ ê¸°ëŠ¥
"""

import asyncio
import json
import logging
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import aiohttp
import aiosqlite
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from serpapi import GoogleSearch
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntelligentWebSearcher:
    """ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self, serpapi_key: str = "YOUR_SERPAPI_KEY_HERE", db_path: str = "search_memory.db"):
        self.serpapi_key = serpapi_key
        self.db_path = db_path
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.dimension = 384  # all-MiniLM-L6-v2ì˜ ì°¨ì›
        self.index = faiss.IndexFlatIP(self.dimension)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©
        self.search_memory = []  # ê²€ìƒ‰ ê¸°ë¡ ë©”íƒ€ë°ì´í„°
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        self.cache = {}
        self.cache_expiry = {}
        self.cache_duration = timedelta(hours=6)  # 6ì‹œê°„ ìºì‹œ
        
        # 2025ë…„ ìµœì‹  ê¸°ìˆ  í‚¤ì›Œë“œ (ê²€ìƒ‰ ê°•í™”ìš©)
        self.tech_keywords_2025 = {
            'ai_models': ['GPT-5', 'Claude 3.5', 'Gemini 2.0', 'Llama 3', 'GPT-4o', 'o1-preview', 'o1-mini'],
            'ai_trends': ['AGI', 'ASI', 'multimodal AI', 'AI agents', 'reasoning models', 'AI safety'],
            'programming': ['GitHub Copilot', 'Cursor IDE', 'AI coding', 'automated programming', 'code generation'],
            'hardware': ['H100', 'H200', 'B200', 'AI chips', 'TPU v5', 'neuromorphic computing'],
            'companies': ['OpenAI', 'Anthropic', 'Google DeepMind', 'Meta AI', 'xAI', 'Perplexity'],
            'frameworks': ['LangChain', 'LlamaIndex', 'AutoGen', 'CrewAI', 'Semantic Kernel']
        }
        
        # 2025ë…„ ìµœì‹  ê²€ìƒ‰ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„
        self.priority_sources_2025 = [
            'openai.com', 'anthropic.com', 'deepmind.google', 'ai.meta.com',
            'github.com', 'arxiv.org', 'papers.nips.cc', 'techcrunch.com',
            'theverge.com', 'wired.com', 'technologyreview.com'
        ]
        
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        await self.init_database()
        await self.load_search_memory()
        logger.info("ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ê¸°ì–µí•˜ë©´ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆì–´ìš”~ âœ¨")
    
    async def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS search_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query TEXT NOT NULL,
                    query_hash TEXT UNIQUE,
                    search_results TEXT,
                    summary TEXT,
                    embedding BLOB,
                    search_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    relevance_score REAL DEFAULT 1.0,
                    access_count INTEGER DEFAULT 1
                )
            """)
            
            await db.execute("""
                CREATE TABLE IF NOT EXISTS search_analytics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query_type TEXT,
                    response_time REAL,
                    result_count INTEGER,
                    user_satisfaction REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            await db.commit()
    
    async def load_search_memory(self):
        """ì €ì¥ëœ ê²€ìƒ‰ ê¸°ë¡ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT query, search_results, summary, embedding, search_date, relevance_score
                    FROM search_history 
                    ORDER BY search_date DESC 
                    LIMIT 1000
                """) as cursor:
                    rows = await cursor.fetchall()
                    
                    embeddings = []
                    for row in rows:
                        query, results, summary, embedding_blob, date, score = row
                        
                        if embedding_blob:
                            # BLOBì—ì„œ numpy ë°°ì—´ë¡œ ë³€í™˜
                            embedding = np.frombuffer(embedding_blob, dtype=np.float32)
                            embeddings.append(embedding)
                            
                            self.search_memory.append({
                                'query': query,
                                'results': json.loads(results) if results else [],
                                'summary': summary,
                                'date': date,
                                'relevance_score': score
                            })
                    
                    if embeddings:
                        embeddings_array = np.vstack(embeddings)
                        self.index.add(embeddings_array.astype('float32'))
                        
            logger.info(f"ê²€ìƒ‰ ê¸°ë¡ {len(self.search_memory)}ê°œ ë¡œë“œ ì™„ë£Œ! ì´ì „ ê¸°ì–µë“¤ì„ ë‹¤ ë¶ˆëŸ¬ì™”ì–´ìš”~ ğŸ§ ")
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê¸°ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def _clean_cache(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        now = datetime.now()
        expired_keys = [
            key for key, expiry in self.cache_expiry.items() 
            if now > expiry
        ]
        
        for key in expired_keys:
            del self.cache[key]
            del self.cache_expiry[key]
    
    def _get_query_hash(self, query: str) -> str:
        """ì¿¼ë¦¬ í•´ì‹œ ìƒì„±"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    async def search_web(self, query: str, num_results: int = 5) -> Dict:
        """SerpAPIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (2025ë…„ ìµœì‹  ì •ë³´ ê°•í™”)"""
        start_time = datetime.now()
        
        try:
            # 2025ë…„ ìµœì‹  ì •ë³´ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ê°•í™”
            enhanced_query = self.enhance_query_for_2025(query)
            
            # ìºì‹œ í™•ì¸ (ì›ë³¸ ì¿¼ë¦¬ë¡œ)
            query_hash = self._get_query_hash(query)
            if query_hash in self.cache and datetime.now() < self.cache_expiry[query_hash]:
                logger.info(f"ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {query}")
                return self.cache[query_hash]
            
            # SerpAPI ê²€ìƒ‰ (ê°•í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©)
            search_params = {
                "q": enhanced_query,
                "api_key": self.serpapi_key,
                "engine": "google",
                "num": num_results,
                "hl": "ko",  # í•œêµ­ì–´
                "gl": "kr",   # í•œêµ­ ì§€ì—­
                "tbs": "qdr:y"  # ìµœê·¼ 1ë…„ ë‚´ ê²°ê³¼ ìš°ì„ 
            }
            
            search = GoogleSearch(search_params)
            results = search.get_dict()
            
            # ê²°ê³¼ ì²˜ë¦¬
            processed_results = self._process_search_results(results)
            
            # ìºì‹œì— ì €ì¥
            self._clean_cache()
            self.cache[query_hash] = processed_results
            self.cache_expiry[query_hash] = datetime.now() + self.cache_duration
            
            # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
            response_time = (datetime.now() - start_time).total_seconds()
            await self._log_search_analytics("web_search", response_time, len(processed_results.get('results', [])))
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'query': query,
                'results': [],
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _process_search_results(self, raw_results: Dict) -> Dict:
        """ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° ì •ë¦¬"""
        processed = {
            'results': [],
            'answer_box': None,
            'knowledge_graph': None,
            'related_questions': [],
            'timestamp': datetime.now().isoformat()
        }
        
        # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼
        if 'organic_results' in raw_results:
            for result in raw_results['organic_results'][:5]:
                processed['results'].append({
                    'title': result.get('title', ''),
                    'link': result.get('link', ''),
                    'snippet': result.get('snippet', ''),
                    'position': result.get('position', 0)
                })
        
        # ë‹µë³€ ë°•ìŠ¤ (ì¦‰ì‹œ ë‹µë³€)
        if 'answer_box' in raw_results:
            answer_box = raw_results['answer_box']
            processed['answer_box'] = {
                'answer': answer_box.get('answer', ''),
                'title': answer_box.get('title', ''),
                'link': answer_box.get('link', '')
            }
        
        # ì§€ì‹ ê·¸ë˜í”„
        if 'knowledge_graph' in raw_results:
            kg = raw_results['knowledge_graph']
            processed['knowledge_graph'] = {
                'title': kg.get('title', ''),
                'description': kg.get('description', ''),
                'attributes': kg.get('attributes', {})
            }
        
        # ê´€ë ¨ ì§ˆë¬¸
        if 'related_questions' in raw_results:
            for rq in raw_results['related_questions'][:3]:
                processed['related_questions'].append({
                    'question': rq.get('question', ''),
                    'snippet': rq.get('snippet', '')
                })
        
        return processed
    
    def enhance_query_for_2025(self, query: str) -> str:
        """2025ë…„ ìµœì‹  ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ê°•í™”"""
        enhanced_query = query
        
        # 2025ë…„ í‚¤ì›Œë“œ ì¶”ê°€
        if any(keyword in query.lower() for keyword in ['ai', 'gpt', 'chatgpt', 'llm', 'ì¸ê³µì§€ëŠ¥']):
            enhanced_query += " 2025 latest"
        
        # íŠ¹ì • ëª¨ë¸ëª… ê°ì§€ ì‹œ ìµœì‹  ë²„ì „ìœ¼ë¡œ í™•ì¥
        model_mappings = {
            'gpt': 'GPT-5 GPT-4o o1-preview',
            'claude': 'Claude 3.5 Sonnet',
            'gemini': 'Gemini 2.0 Pro',
            'llama': 'Llama 3.1 3.2'
        }
        
        for old_term, new_terms in model_mappings.items():
            if old_term in query.lower():
                enhanced_query += f" {new_terms}"
        
        # ìµœì‹  ì†ŒìŠ¤ ìš°ì„  ê²€ìƒ‰ì„ ìœ„í•œ site í•„í„° ì¶”ê°€ (ê°€ë”ì”©)
        import random
        if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ìš°ì„  ì†ŒìŠ¤ ê²€ìƒ‰
            priority_site = random.choice(self.priority_sources_2025)
            enhanced_query += f" site:{priority_site}"
        
        return enhanced_query
    
    async def generate_summary(self, search_results: Dict, query: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ ìƒì„±"""
        try:
            summary_parts = []
            
            # ë‹µë³€ ë°•ìŠ¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if search_results.get('answer_box'):
                answer = search_results['answer_box'].get('answer', '')
                if answer:
                    summary_parts.append(f"ğŸ“‹ ë°”ë¡œ ë‹µë³€ë“œë¦´ê²Œìš”: {answer}")
            
            # ì§€ì‹ ê·¸ë˜í”„ ì •ë³´
            if search_results.get('knowledge_graph'):
                kg = search_results['knowledge_graph']
                if kg.get('description'):
                    summary_parts.append(f"ğŸ“š ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´: {kg['description']}")
            
            # ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            if search_results.get('results'):
                summary_parts.append("ğŸ” ì°¾ì•„ë³¸ ì£¼ìš” ë‚´ìš©ë“¤:")
                for i, result in enumerate(search_results['results'][:3], 1):
                    title = result.get('title', '')
                    snippet = result.get('snippet', '')
                    if title and snippet:
                        summary_parts.append(f"{i}. **{title}**\n   {snippet[:150]}...")
            
            # ê´€ë ¨ ì§ˆë¬¸
            if search_results.get('related_questions'):
                summary_parts.append("\nâ“ ì´ëŸ° ê²ƒë„ ê¶ê¸ˆí•˜ì‹¤ ê²ƒ ê°™ì•„ìš”:")
                for rq in search_results['related_questions']:
                    question = rq.get('question', '')
                    if question:
                        summary_parts.append(f"â€¢ {question}")
            
            return "\n\n".join(summary_parts) if summary_parts else "ì–´ë¼? ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê¸°ê°€ ì–´ë ¤ì›Œìš”... ğŸ˜…"
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def save_search_to_memory(self, query: str, search_results: Dict, summary: str):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¥ê¸° ê¸°ì–µì— ì €ì¥"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.encoder.encode([query])[0]
            query_embedding = query_embedding / np.linalg.norm(query_embedding)  # ì •ê·œí™”
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            query_hash = self._get_query_hash(query)
            
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT OR REPLACE INTO search_history 
                    (query, query_hash, search_results, summary, embedding, search_date, relevance_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    query,
                    query_hash,
                    json.dumps(search_results, ensure_ascii=False),
                    summary,
                    query_embedding.tobytes(),
                    datetime.now().isoformat(),
                    1.0
                ))
                await db.commit()
            
            # ë©”ëª¨ë¦¬ì—ë„ ì¶”ê°€
            self.search_memory.append({
                'query': query,
                'results': search_results,
                'summary': summary,
                'date': datetime.now().isoformat(),
                'relevance_score': 1.0
            })
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(query_embedding.reshape(1, -1).astype('float32'))
            
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {query} (ì´ì œ ê¸°ì–µí•´ë’€ì–´ìš”! ğŸ’¾)")
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    async def find_related_memory(self, query: str, top_k: int = 3, similarity_threshold: float = 0.7) -> List[Dict]:
        """ê´€ë ¨ëœ ê³¼ê±° ê²€ìƒ‰ ê¸°ë¡ ì°¾ê¸°"""
        try:
            if len(self.search_memory) == 0:
                return []
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.encoder.encode([query])[0]
            query_embedding = query_embedding / np.linalg.norm(query_embedding)
            
            # FAISSë¡œ ìœ ì‚¬í•œ ê²€ìƒ‰ ì°¾ê¸°
            scores, indices = self.index.search(
                query_embedding.reshape(1, -1).astype('float32'), 
                min(top_k, len(self.search_memory))
            )
            
            related_memories = []
            for score, idx in zip(scores[0], indices[0]):
                if score >= similarity_threshold and idx < len(self.search_memory):
                    memory = self.search_memory[idx].copy()
                    memory['similarity_score'] = float(score)
                    related_memories.append(memory)
            
            return sorted(related_memories, key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def intelligent_search(self, query: str) -> Dict:
        """ì§€ëŠ¥í˜• ê²€ìƒ‰ - ê¸°ì–µ í™•ì¸ í›„ í•„ìš”ì‹œ ìƒˆë¡œ ê²€ìƒ‰"""
        try:
            # 1. ê´€ë ¨ëœ ê³¼ê±° ê²€ìƒ‰ ê¸°ë¡ í™•ì¸
            related_memories = await self.find_related_memory(query, top_k=3, similarity_threshold=0.8)
            
            # 2. ìµœê·¼ ê´€ë ¨ ê²€ìƒ‰ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
            if related_memories:
                recent_memory = related_memories[0]
                # 24ì‹œê°„ ì´ë‚´ì˜ ê²€ìƒ‰ì´ë©´ ì¬ì‚¬ìš©
                memory_date = datetime.fromisoformat(recent_memory['date'].replace('Z', '+00:00').replace('+00:00', ''))
                if datetime.now() - memory_date < timedelta(hours=24):
                    logger.info(f"ê¸°ì¡´ ê²€ìƒ‰ ê¸°ë¡ í™œìš©: {query} (ì•„! ì´ê±° ì „ì— ì°¾ì•„ë´¤ë˜ ê±°ë„¤ìš”~)")
                    return {
                        'type': 'memory_based',
                        'query': query,
                        'answer': recent_memory['summary'],
                        'source': 'previous_search',
                        'similarity_score': recent_memory['similarity_score'],
                        'original_query': recent_memory['query'],
                        'search_date': recent_memory['date']
                    }
            
            # 3. ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰
            logger.info(f"ìƒˆë¡œìš´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query} (ìƒˆë¡œ ì°¾ì•„ë³¼ê²Œìš”! ğŸ”)")
            search_results = await self.search_web(query)
            
            if search_results.get('error'):
                return {
                    'type': 'error',
                    'query': query,
                    'error': search_results['error']
                }
            
            # 4. ìš”ì•½ ìƒì„±
            summary = await self.generate_summary(search_results, query)
            
            # 5. ê¸°ì–µì— ì €ì¥
            await self.save_search_to_memory(query, search_results, summary)
            
            # 6. ê´€ë ¨ ê¸°ì–µê³¼ í•¨ê»˜ ê²°ê³¼ ë°˜í™˜
            return {
                'type': 'new_search',
                'query': query,
                'answer': summary,
                'search_results': search_results,
                'related_memories': related_memories[:2] if related_memories else [],
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì§€ëŠ¥í˜• ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'type': 'error',
                'query': query,
                'error': str(e)
            }
    
    async def _log_search_analytics(self, query_type: str, response_time: float, result_count: int, satisfaction: float = 1.0):
        """ê²€ìƒ‰ ë¶„ì„ ë°ì´í„° ë¡œê¹…"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT INTO search_analytics 
                    (query_type, response_time, result_count, user_satisfaction)
                    VALUES (?, ?, ?, ?)
                """, (query_type, response_time, result_count, satisfaction))
                await db.commit()
        except Exception as e:
            logger.error(f"ë¶„ì„ ë°ì´í„° ë¡œê¹… ì˜¤ë¥˜: {e}")
    
    async def get_search_stats(self) -> Dict:
        """ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # ì´ ê²€ìƒ‰ ìˆ˜
                async with db.execute("SELECT COUNT(*) FROM search_history") as cursor:
                    total_searches = (await cursor.fetchone())[0]
                
                # ìµœê·¼ 24ì‹œê°„ ê²€ìƒ‰ ìˆ˜
                async with db.execute("""
                    SELECT COUNT(*) FROM search_history 
                    WHERE search_date > datetime('now', '-1 day')
                """) as cursor:
                    recent_searches = (await cursor.fetchone())[0]
                
                # í‰ê·  ì‘ë‹µ ì‹œê°„
                async with db.execute("""
                    SELECT AVG(response_time) FROM search_analytics 
                    WHERE created_at > datetime('now', '-7 days')
                """) as cursor:
                    avg_response_time = (await cursor.fetchone())[0] or 0
                
                return {
                    'total_searches': total_searches,
                    'recent_searches': recent_searches,
                    'avg_response_time': round(avg_response_time, 2),
                    'memory_size': len(self.search_memory),
                    'cache_size': len(self.cache)
                }
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {}

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
web_searcher = None

async def initialize_web_search(serpapi_key: str = "YOUR_SERPAPI_KEY_HERE"):
    """ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global web_searcher
    web_searcher = IntelligentWebSearcher(serpapi_key)
    await web_searcher.initialize()
    return web_searcher

async def search_and_remember(query: str) -> Dict:
    """ê²€ìƒ‰í•˜ê³  ê¸°ì–µí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    if web_searcher is None:
        return {
            'type': 'error',
            'error': 'ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
        }
    
    return await web_searcher.intelligent_search(query)

async def get_search_statistics() -> Dict:
    """ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
    if web_searcher is None:
        return {}
    
    return await web_searcher.get_search_stats()