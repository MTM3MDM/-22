"""
ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ë° ê¸°ì–µ ì‹œìŠ¤í…œ
SerpAPIë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ê²€ìƒ‰ê³¼ ë²¡í„° ê¸°ë°˜ ê¸°ì–µ ê¸°ëŠ¥
"""

import asyncio
import json
import logging
import hashlib
from datetime import datetime, timedelta, timezone
import aiohttp
import aiosqlite
import numpy as np
from numpy.typing import NDArray
from sentence_transformers import SentenceTransformer
import faiss  # type: ignore
from serpapi import GoogleSearch  # type: ignore
import re
from typing import Any, Dict, List, Optional, cast

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntelligentWebSearcher:
    """ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self, serpapi_key: str, db_path: str = "search_memory.db"):
        self.serpapi_key = serpapi_key
        self.db_path = db_path
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.dimension = 384  # all-MiniLM-L6-v2ì˜ ì°¨ì›
        self.index: faiss.Index = faiss.IndexFlatIP(self.dimension)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©
        self.search_memory: List[Dict[str, Any]] = []  # ê²€ìƒ‰ ê¸°ë¡ ë©”íƒ€ë°ì´í„°
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        self.cache: Dict[str, Any] = {}
        self.cache_expiry: Dict[str, datetime] = {}
        self.cache_duration = timedelta(hours=6)  # 6ì‹œê°„ ìºì‹œ
        
        # 2025ë…„ ìµœì‹  ê¸°ìˆ  í‚¤ì›Œë“œ (ê²€ìƒ‰ ê°•í™”ìš©)
        self.tech_keywords_2025 = {
            'ai_models': ['GPT-5', 'Claude 3.5', 'Gemini 2.0', 'Llama 3', 'GPT-4o', 'o1-preview', 'o1-mini'],
            'ai_trends': ['AGI', 'ASI', 'multimodal AI', 'AI agents', 'reasoning models', 'AI safety'],
            'programming': ['GitHub Copilot', 'Cursor IDE', 'AI coding', 'automated programming', 'code generation'],
            'hardware': ['H100', 'H200', 'B200', 'AI chips', 'TPU v5', 'neuromorphic computing'],
            'companies': ['OpenAI', 'Anthropic', 'Google DeepMind', 'Meta AI', 'xAI', 'Perplexity'],
            'frameworks': ['LangChain', 'LlamaIndex', 'AutoGen', 'CrewAI', 'Semantic Kernel']
        }
        
        # 2025ë…„ ìµœì‹  ê²€ìƒ‰ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„
        self.priority_sources_2025 = [
            'openai.com', 'anthropic.com', 'deepmind.google', 'ai.meta.com',
            'github.com', 'arxiv.org', 'papers.nips.cc', 'techcrunch.com',
            'theverge.com', 'wired.com', 'technologyreview.com'
        ]
        
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        await self.init_database()
        await self.load_search_memory()
        logger.info("ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ê¸°ì–µí•˜ë©´ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆì–´ìš”~ âœ¨")
    
    async def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS search_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query TEXT NOT NULL,
                    query_hash TEXT UNIQUE,
                    search_results TEXT,
                    summary TEXT,
                    embedding BLOB,
                    search_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    relevance_score REAL DEFAULT 1.0,
                    access_count INTEGER DEFAULT 1
                )
            """)
            
            await db.execute("""
                CREATE TABLE IF NOT EXISTS search_analytics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query_type TEXT,
                    response_time REAL,
                    result_count INTEGER,
                    user_satisfaction REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            await db.commit()
    
    async def load_search_memory(self):
        """ì €ì¥ëœ ê²€ìƒ‰ ê¸°ë¡ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute("""
                    SELECT query, search_results, summary, embedding, search_date, relevance_score
                    FROM search_history 
                    ORDER BY search_date DESC 
                    LIMIT 1000
                """) as cursor:
                    rows = await cursor.fetchall()
                    
                    embeddings: List[NDArray[np.float32]] = []
                    for row in rows:
                        query, results, summary, embedding_blob, date, score = row
                        
                        if embedding_blob:
                            embedding: NDArray[np.float32] = np.frombuffer(embedding_blob, dtype=np.float32)
                            embeddings.append(embedding)
                            
                            self.search_memory.append({
                                'query': query,
                                'results': json.loads(results) if results else [],
                                'summary': summary,
                                'date': date,
                                'relevance_score': score
                            })
                    
                    if embeddings:
                        embeddings_array: NDArray[np.float32] = np.vstack(embeddings).astype('float32')
                        self.index.add(embeddings_array)  # type: ignore
                        
            logger.info(f"ê²€ìƒ‰ ê¸°ë¡ {len(self.search_memory)}ê°œ ë¡œë“œ ì™„ë£Œ! ì´ì „ ê¸°ì–µë“¤ì„ ë‹¤ ë¶ˆëŸ¬ì™”ì–´ìš”~ ğŸ§ ")
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê¸°ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def _clean_cache(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        now = datetime.now()
        expired_keys = [
            key for key, expiry in self.cache_expiry.items() 
            if now > expiry
        ]
        
        for key in expired_keys:
            self.cache.pop(key, None)
            self.cache_expiry.pop(key, None)
    
    def _get_query_hash(self, query: str) -> str:
        """ì¿¼ë¦¬ í•´ì‹œ ìƒì„±"""
        return hashlib.md5(query.lower().strip().encode()).hexdigest()
    
    async def search_web(self, query: str, num_results: int = 5) -> Dict[str, Any]:
        """SerpAPIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (2025ë…„ ìµœì‹  ì •ë³´ ê°•í™”)"""
        start_time = datetime.now()
        
        try:
            async with aiohttp.ClientSession() as session:
                # ì¿¼ë¦¬ ê°•í™”
                enhanced_query = self.enhance_query_for_2025(query)
                
                # ìºì‹œ í™•ì¸ (ì›ë³¸ ì¿¼ë¦¬ë¡œ)
                query_hash = self._get_query_hash(query)
                expiry_time = self.cache_expiry.get(query_hash, datetime.min)
                if query_hash in self.cache and datetime.now() < expiry_time:
                    logger.info(f"ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {query}")
                    return self.cache[query_hash]
                
                # SerpAPI ê²€ìƒ‰ (ê°•í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©)
                search_params: Dict[str, Any] = {
                    "api_key": self.serpapi_key,
                    "engine": "google",
                    "q": enhanced_query,
                    "hl": "ko",  # í•œêµ­ì–´
                    "gl": "kr",   # í•œêµ­ ì§€ì—­
                    "tbs": "qdr:y",  # ìµœê·¼ 1ë…„ ë‚´ ê²°ê³¼ ìš°ì„ 
                    "num": str(num_results)
                }
                
                search = GoogleSearch(search_params)
                loop = asyncio.get_event_loop()
                raw_results = await loop.run_in_executor(None, search.get_dict)
                
            # ê²°ê³¼ ì²˜ë¦¬
            processed_results = self._process_search_results(raw_results)
            
            # ìºì‹œì— ì €ì¥
            self._clean_cache()
            self.cache[query_hash] = processed_results
            self.cache_expiry[query_hash] = datetime.now() + self.cache_duration
            
            # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
            response_time = (datetime.now() - start_time).total_seconds()
            await self._log_search_analytics("web_search", response_time, len(processed_results.get('results', [])))
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'query': query,
                'results': [],
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _process_search_results(self, raw_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° ì •ë¦¬"""
        processed: Dict[str, Any] = {
            'results': [],
            'answer_box': None,
            'knowledge_graph': None,
            'related_questions': []
        }
        
        # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼
        organic_results = raw_results.get('organic_results', [])
        if isinstance(organic_results, list):
            for result in organic_results[:5]:
                if isinstance(result, dict):
                    processed['results'].append({
                        'title': str(result.get('title', '')),
                        'link': str(result.get('link', '')),
                        'snippet': str(result.get('snippet', '')),
                        'position': int(result.get('position', 0))
                    })
        
        # ë‹µë³€ ë°•ìŠ¤ (ì¦‰ì‹œ ë‹µë³€)
        answer_box_data = raw_results.get('answer_box')
        if isinstance(answer_box_data, dict):
            processed['answer_box'] = {
                'answer': str(answer_box_data.get('answer', '')),
                'title': str(answer_box_data.get('title', '')),
                'link': str(answer_box_data.get('link', ''))
            }
        
        # ì§€ì‹ ê·¸ë˜í”„
        kg_data = raw_results.get('knowledge_graph')
        if isinstance(kg_data, dict):
            processed['knowledge_graph'] = {
                'title': str(kg_data.get('title', '')),
                'description': str(kg_data.get('description', '')),
                'attributes': cast(Dict[str, Any], kg_data.get('attributes', {}))
            }
        
        # ê´€ë ¨ ì§ˆë¬¸
        related_questions_data = raw_results.get('related_questions', [])
        if isinstance(related_questions_data, list):
            for rq_item in related_questions_data[:3]:
                if isinstance(rq_item, dict):
                    processed['related_questions'].append({
                        'question': str(rq_item.get('question', '')),
                        'snippet': str(rq_item.get('snippet', ''))
                    })
        
        return processed
    
    def enhance_query_for_2025(self, query: str) -> str:
        """2025ë…„ ìµœì‹  ì •ë³´ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ê°•í™”"""
        enhanced_query = query
        
        # 2025ë…„ í‚¤ì›Œë“œ ì¶”ê°€
        if any(keyword in query.lower() for keyword in ['ai', 'gpt', 'chatgpt', 'llm', 'ì¸ê³µì§€ëŠ¥']):
            enhanced_query += " 2025 latest"
        
        # íŠ¹ì • ëª¨ë¸ëª… ê°ì§€ ì‹œ ìµœì‹  ë²„ì „ìœ¼ë¡œ í™•ì¥
        model_mappings = {
            'gpt': 'GPT-5 GPT-4o o1-preview',
            'claude': 'Claude 3.5 Sonnet',
            'gemini': 'Gemini 2.0 Pro',
            'llama': 'Llama 3.1 3.2'
        }
        
        for old_term, new_terms in model_mappings.items():
            if old_term in query.lower():
                enhanced_query += f" {new_terms}"
        
        # ìµœì‹  ì†ŒìŠ¤ ìš°ì„  ê²€ìƒ‰ì„ ìœ„í•œ site í•„í„° ì¶”ê°€ (ê°€ë”ì”©)
        import random
        if random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ìš°ì„  ì†ŒìŠ¤ ê²€ìƒ‰
            priority_site = random.choice(self.priority_sources_2025)
            enhanced_query += f" site:{priority_site}"
        
        return enhanced_query
    
    async def generate_summary(self, search_results: Dict[str, Any], query: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ ìƒì„±"""
        try:
            summary_parts: List[str] = []
            
            # ë‹µë³€ ë°•ìŠ¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            answer_box = search_results.get('answer_box')
            if isinstance(answer_box, dict):
                answer = str(answer_box.get('answer', ''))
                if answer:
                    summary_parts.append(f"ğŸ“‹ ë°”ë¡œ ë‹µë³€ë“œë¦´ê²Œìš”: {answer}")
            
            # ì§€ì‹ ê·¸ë˜í”„ ì •ë³´
            knowledge_graph = search_results.get('knowledge_graph')
            if isinstance(knowledge_graph, dict):
                description = str(knowledge_graph.get('description', ''))
                if description:
                    summary_parts.append(f"ğŸ“š ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´: {description}")
            
            # ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            results = search_results.get('results')
            if isinstance(results, list) and results:
                summary_parts.append("ğŸ” ì°¾ì•„ë³¸ ì£¼ìš” ë‚´ìš©ë“¤:")
                for i, result in enumerate(results[:3], 1):
                    if isinstance(result, dict):
                        title = str(result.get('title', ''))
                        snippet = str(result.get('snippet', ''))
                        if title and snippet:
                            summary_parts.append(f"{i}. **{title}**\n   {snippet[:150]}...")
            
            # ê´€ë ¨ ì§ˆë¬¸
            related_questions = search_results.get('related_questions')
            if isinstance(related_questions, list) and related_questions:
                summary_parts.append("\nâ“ ì´ëŸ° ê²ƒë„ ê¶ê¸ˆí•˜ì‹¤ ê²ƒ ê°™ì•„ìš”:")
                for rq in related_questions:
                    if isinstance(rq, dict):
                        question = str(rq.get('question', ''))
                        if question:
                            summary_parts.append(f"â€¢ {question}")
            
            return "\n\n".join(summary_parts) if summary_parts else "ì–´ë¼? ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê¸°ê°€ ì–´ë ¤ì›Œìš”... ğŸ˜…"
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def save_search_to_memory(self, query: str, search_results: Dict[str, Any], summary: str):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¥ê¸° ê¸°ì–µì— ì €ì¥"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding_unnormalized = self.encoder.encode([query])[0]
            query_embedding: NDArray[np.float32] = query_embedding_unnormalized / np.linalg.norm(query_embedding_unnormalized)  # ì •ê·œí™”
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            query_hash = self._get_query_hash(query)
            
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT OR REPLACE INTO search_history 
                    (query, query_hash, search_results, summary, embedding, search_date, relevance_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    query,
                    query_hash,
                    json.dumps(search_results, ensure_ascii=False),
                    summary,
                    query_embedding.tobytes(),
                    datetime.now().isoformat(),
                    1.0
                ))
                await db.commit()
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(query_embedding.reshape(1, -1).astype('float32'))  # type: ignore
            
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {query} (ì´ì œ ê¸°ì–µí•´ë’€ì–´ìš”! ğŸ’¾)")
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    async def find_related_memory(self, query: str, top_k: int = 3, similarity_threshold: float = 0.7) -> List[Dict[str, Any]]:
        """ê´€ë ¨ëœ ê³¼ê±° ê²€ìƒ‰ ê¸°ë¡ ì°¾ê¸°"""
        try:
            if self.index.ntotal == 0:
                return []
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding_unnormalized = self.encoder.encode([query])[0]
            query_embedding: NDArray[np.float32] = query_embedding_unnormalized / np.linalg.norm(query_embedding_unnormalized)
            
            # FAISSë¡œ ìœ ì‚¬í•œ ê²€ìƒ‰ ì°¾ê¸°
            scores, indices = self.index.search(  # type: ignore
                query_embedding.reshape(1, -1).astype('float32'), 
                min(top_k, self.index.ntotal)
            )
            
            related_memories = []
            for score, idx in zip(scores[0], indices[0]):
                if score >= similarity_threshold and idx < len(self.search_memory):
                    memory = self.search_memory[idx].copy()
                    memory['similarity_score'] = float(score)
                    related_memories.append(memory)
            
            return sorted(related_memories, key=lambda x: x['similarity_score'], reverse=True)
            
        except Exception as e:
            logger.error(f"ê´€ë ¨ ê¸°ì–µ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def intelligent_search(self, query: str) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ê²€ìƒ‰ - ê¸°ì–µ í™•ì¸ í›„ í•„ìš”ì‹œ ìƒˆë¡œ ê²€ìƒ‰"""
        try:
            # 1. ê´€ë ¨ëœ ê³¼ê±° ê²€ìƒ‰ ê¸°ë¡ í™•ì¸
            related_memories = await self.find_related_memory(query, top_k=3, similarity_threshold=0.8)
            
            # 2. ìµœê·¼ ê´€ë ¨ ê²€ìƒ‰ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
            if related_memories:
                recent_memory = related_memories[0]
                # 24ì‹œê°„ ì´ë‚´ì˜ ê²€ìƒ‰ì´ë©´ ì¬ì‚¬ìš©
                memory_date_str = recent_memory.get('date', '')
                if memory_date_str:
                    try:
                        # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° UTCë¡œ ê°„ì£¼
                        memory_date = datetime.fromisoformat(memory_date_str.replace('Z', '+00:00'))
                        if memory_date.tzinfo is None:
                            memory_date = memory_date.replace(tzinfo=timezone.utc)
                            
                        if datetime.now(timezone.utc) - memory_date < timedelta(hours=24):
                            logger.info(f"ê¸°ì¡´ ê²€ìƒ‰ ê¸°ë¡ í™œìš©: {query} (ì•„! ì´ê±° ì „ì— ì°¾ì•„ë´¤ë˜ ê±°ë„¤ìš”~)")
                            return {
                                'type': 'memory_based',
                                'query': query,
                                'answer': recent_memory.get('summary', ''),
                                'source': 'previous_search',
                                'similarity_score': recent_memory.get('similarity_score', 0.0),
                                'original_query': recent_memory.get('query', ''),
                                'search_date': memory_date_str
                            }
                    except ValueError:
                        logger.warning(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {memory_date_str}")

            # 3. ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰
            logger.info(f"ìƒˆë¡œìš´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query} (ìƒˆë¡œ ì°¾ì•„ë³¼ê²Œìš”! ğŸ”)")
            search_results = await self.search_web(query)
            
            if search_results.get('error'):
                return {
                    'type': 'error',
                    'query': query,
                    'error': search_results['error']
                }
            
            # 4. ìš”ì•½ ìƒì„±
            summary = await self.generate_summary(search_results, query)
            
            # 5. ê¸°ì–µì— ì €ì¥
            await self.save_search_to_memory(query, search_results, summary)
            
            # 6. ê´€ë ¨ ê¸°ì–µê³¼ í•¨ê»˜ ê²°ê³¼ ë°˜í™˜
            return {
                'type': 'new_search',
                'query': query,
                'answer': summary,
                'search_results': search_results,
                'related_memories': related_memories[:2] if related_memories else [],
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì§€ëŠ¥í˜• ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                'type': 'error',
                'query': query,
                'error': str(e)
            }
    
    async def _log_search_analytics(self, query_type: str, response_time: float, result_count: int, satisfaction: float = 1.0):
        """ê²€ìƒ‰ ë¶„ì„ ë°ì´í„° ë¡œê¹…"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute("""
                    INSERT INTO search_analytics 
                    (query_type, response_time, result_count, user_satisfaction)
                    VALUES (?, ?, ?, ?)
                """, (query_type, response_time, result_count, satisfaction))
                await db.commit()
        except Exception as e:
            logger.error(f"ë¶„ì„ ë°ì´í„° ë¡œê¹… ì˜¤ë¥˜: {e}")
    
    async def get_search_stats(self) -> Dict:
        """ê²€ìƒ‰ í†µê³„ ì¡°íšŒ"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # ì´ ê²€ìƒ‰ ìˆ˜
                async with db.execute("SELECT COUNT(*) FROM search_history") as cursor:
                    total_searches = (await cursor.fetchone())[0]
                
                # ìµœê·¼ 24ì‹œê°„ ê²€ìƒ‰ ìˆ˜
                async with db.execute("""
                    SELECT COUNT(*) FROM search_history 
                    WHERE search_date > datetime('now', '-1 day')
                """) as cursor:
                    recent_searches = (await cursor.fetchone())[0]
                
                # í‰ê·  ì‘ë‹µ ì‹œê°„
                async with db.execute("""
                    SELECT AVG(response_time) FROM search_analytics 
                    WHERE created_at > datetime('now', '-7 days')
                """) as cursor:
                    avg_response_time = (await cursor.fetchone())[0] or 0
                
                return {
                    'total_searches': total_searches,
                    'recent_searches': recent_searches,
                    'avg_response_time': round(avg_response_time, 2),
                    'memory_size': len(self.search_memory),
                    'cache_size': len(self.cache)
                }
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {}

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_web_searcher_instance: Optional[IntelligentWebSearcher] = None

async def initialize_web_search(serpapi_key: str):
    """
    ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    SerpAPI í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """
    global _web_searcher_instance
    if _web_searcher_instance is None:
        if not serpapi_key:
            logger.error("SerpAPI í‚¤ê°€ ì œê³µë˜ì§€ ì•Šì•„ ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        _web_searcher_instance = IntelligentWebSearcher(serpapi_key=serpapi_key)
        await _web_searcher_instance.initialize()
    logger.info("ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
async def initialize_intelligent_search(serpapi_key: str):
    """initialize_web_searchì˜ ë³„ì¹­ (í˜¸í™˜ì„± ìœ ì§€)"""
    await initialize_web_search(serpapi_key)

async def search_and_remember(query: str) -> Dict[str, Any]:
    """
    ì§€ëŠ¥í˜• ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê³¼ê±° ê²€ìƒ‰ ê¸°ë¡ì„ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ìƒˆë¡œìš´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤.
    """
    if _web_searcher_instance is None:
        logger.warning("ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € initialize_web_searchë¥¼ í˜¸ì¶œí•´ì£¼ì„¸ìš”.")
        return {
            'type': 'error',
            'query': query,
            'error': 'Web search system is not initialized. SERPAPI_KEY might be missing.'
        }
    
    return await _web_searcher_instance.intelligent_search(query)

async def get_search_statistics() -> Dict[str, Any]:
    """
    ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if _web_searcher_instance is None:
        logger.warning("ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ í†µê³„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
        
    return await _web_searcher_instance.get_search_stats()

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë”ë¯¸ í•¨ìˆ˜
async def process_and_learn(data: str) -> str:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ ë”ë¯¸ í•¨ìˆ˜"""
    logger.warning("process_and_learn í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. get_smart_answerë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    return "ì´ ê¸°ëŠ¥ì€ í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."