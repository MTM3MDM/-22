# ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ - AI/GPT/ê¸°ìˆ  ë‰´ìŠ¤ ì „ë¬¸ ì²˜ë¦¬
# ë²¡í„° ê²€ìƒ‰, ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘, ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±

import asyncio
import aiohttp
import json
import logging
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import hashlib
import pickle
from pathlib import Path

# ë²¡í„° ê²€ìƒ‰ ë° ì„ë² ë”©
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

# ë‰´ìŠ¤ ë° ë°ì´í„° ìˆ˜ì§‘
import feedparser
import requests
from bs4 import BeautifulSoup
import newspaper
from newspaper import Article

# í…ìŠ¤íŠ¸ ì²˜ë¦¬
import re
from collections import defaultdict, Counter
import tiktoken

logger = logging.getLogger(__name__)

class TechNewsCollector:
    """ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.news_sources = {
            'ai_news_2025': [
                'https://feeds.feedburner.com/oreilly/radar',
                'https://techcrunch.com/category/artificial-intelligence/feed/',
                'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
                'https://venturebeat.com/ai/feed/',
                'https://www.wired.com/feed/tag/ai/latest/rss',
                'https://spectrum.ieee.org/rss/blog/artificial-intelligence',
                'https://www.technologyreview.com/feed/',
                'https://www.artificialintelligence-news.com/feed/',
                'https://machinelearningmastery.com/feed/',
                'https://towardsdatascience.com/feed',
                'https://distill.pub/rss.xml',
            ],
            'gpt_llm_2025': [
                'https://openai.com/blog/rss.xml',
                'https://blog.google/technology/ai/rss/',
                'https://blogs.microsoft.com/ai/feed/',
                'https://www.anthropic.com/news/rss.xml',
                'https://huggingface.co/blog/feed.xml',
                'https://deepmind.google/discover/blog/rss.xml',
                'https://stability.ai/news/rss',
                'https://cohere.com/blog/rss.xml',
            ],
            'tech_startup_2025': [
                'https://feeds.feedburner.com/TechCrunch',
                'https://www.engadget.com/rss.xml',
                'https://arstechnica.com/feed/',
                'https://www.zdnet.com/news/rss.xml',
                'https://www.cnet.com/rss/news/',
                'https://techcrunch.com/category/startups/feed/',
                'https://www.producthunt.com/feed',
                'https://news.ycombinator.com/rss',
                'https://www.crunchbase.com/feed',
            ],
            'dev_programming_2025': [
                'https://github.blog/feed/',
                'https://stackoverflow.blog/feed/',
                'https://dev.to/feed',
                'https://www.freecodecamp.org/news/rss/',
                'https://css-tricks.com/feed/',
                'https://www.smashingmagazine.com/feed/',
                'https://blog.jetbrains.com/feed/',
            ],
            'korean_tech_2025': [
                'https://www.etnews.com/rss/S08.xml',
                'https://www.bloter.net/feed',
                'https://www.itworld.co.kr/rss/news.xml',
                'https://zdnet.co.kr/rss/news.xml',
                'https://www.boannews.com/rss/news.xml',
                'https://www.ddaily.co.kr/rss/S1N15.xml',
            ],
            'cloud_blockchain_2025': [
                'https://aws.amazon.com/blogs/aws/feed/',
                'https://cloud.google.com/blog/rss',
                'https://azure.microsoft.com/en-us/blog/feed/',
                'https://www.coindesk.com/arc/outboundfeeds/rss/',
                'https://cointelegraph.com/rss',
                'https://decrypt.co/feed',
            ]
        }
        
        self.session = None
        self.collected_articles = []
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def collect_from_rss(self, url: str, category: str) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    articles = []
                    for entry in feed.entries[:10]:  # ìµœì‹  10ê°œë§Œ
                        article = {
                            'title': entry.get('title', ''),
                            'summary': entry.get('summary', ''),
                            'link': entry.get('link', ''),
                            'published': entry.get('published', ''),
                            'category': category,
                            'source': feed.feed.get('title', 'Unknown'),
                            'collected_at': datetime.now().isoformat()
                        }
                        articles.append(article)
                    
                    logger.info(f"RSSì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘: {url}")
                    return articles
                    
        except Exception as e:
            logger.error(f"RSS ìˆ˜ì§‘ ì˜¤ë¥˜ {url}: {e}")
            return []
    
    async def collect_all_news(self) -> List[Dict]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []
        
        # ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        ) as session:
            self.session = session
            
            for category, urls in self.news_sources.items():
                for url in urls:
                    articles = await self.collect_from_rss(url, category)
                    if articles is not None:  # articlesê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ í™•ì¥
                        all_articles.extend(articles)
                    await asyncio.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        seen_titles = set()
        unique_articles = []
        for article in all_articles:
            if article is not None and article.get('title'):  # articleê³¼ titleì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                try:
                    title_hash = hashlib.md5(article['title'].encode()).hexdigest()
                    if title_hash not in seen_titles:
                        seen_titles.add(title_hash)
                        unique_articles.append(article)
                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    # í•´ì‹œ ìƒì„±ì— ì‹¤íŒ¨í•´ë„ ê¸°ì‚¬ëŠ” ì¶”ê°€
                    unique_articles.append(article)
        
        logger.info(f"ì´ {len(unique_articles)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles
    
    async def enhance_article_content(self, article: Dict) -> Dict:
        """ê¸°ì‚¬ ë‚´ìš© ìƒì„¸ ìˆ˜ì§‘"""
        try:
            if not article.get('link'):
                return article
                
            async with self.session.get(article['link']) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # newspaper3kë¡œ ë³¸ë¬¸ ì¶”ì¶œ
                    try:
                        news_article = Article(article['link'])
                        news_article.set_html(html_content)
                        news_article.parse()
                        
                        article['full_text'] = news_article.text
                        article['authors'] = news_article.authors
                        article['keywords'] = news_article.keywords
                        
                    except Exception as e:
                        logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {article['link']}: {e}")
                        
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ì˜¤ë¥˜ {article['link']}: {e}")
            
        return article

class VectorKnowledgeBase:
    """ë²¡í„° ê¸°ë°˜ ì§€ì‹ë² ì´ìŠ¤"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.encoder = None
        self.index = None
        self.documents = []
        self.metadata = []
        self.dimension = 384  # all-MiniLM-L6-v2ì˜ ì°¨ì›
        
        # ChromaDB ì„¤ì •
        self.chroma_client = None
        self.collection = None
        
        self.data_dir = Path("knowledge_base")
        self.data_dir.mkdir(exist_ok=True)
        
    async def initialize(self):
        """ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # SentenceTransformer ëª¨ë¸ ë¡œë“œ
            self.encoder = SentenceTransformer(self.model_name)
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
            
            # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            
            # ChromaDB ì´ˆê¸°í™”
            self.chroma_client = chromadb.PersistentClient(
                path=str(self.data_dir / "chroma_db")
            )
            
            try:
                self.collection = self.chroma_client.get_collection("tech_knowledge")
                logger.info("ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ")
            except:
                self.collection = self.chroma_client.create_collection(
                    name="tech_knowledge",
                    metadata={"description": "Tech news and AI knowledge base"}
                )
                logger.info("ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„±")
                
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            await self.load_existing_data()
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise
    
    async def load_existing_data(self):
        """ê¸°ì¡´ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index_path = self.data_dir / "faiss_index.bin"
            if index_path.exists():
                self.index = faiss.read_index(str(index_path))
                logger.info("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = self.data_dir / "metadata.pkl"
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    self.metadata = pickle.load(f)
                logger.info(f"ë©”íƒ€ë°ì´í„° {len(self.metadata)}ê°œ ë¡œë“œ")
                
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    async def save_data(self):
        """ë°ì´í„° ì €ì¥"""
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, str(self.data_dir / "faiss_index.bin"))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.data_dir / "metadata.pkl", 'wb') as f:
                pickle.dump(self.metadata, f)
                
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
            
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    async def add_articles(self, articles: List[Dict]):
        """ê¸°ì‚¬ë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€"""
        if not articles:
            return
            
        try:
            texts_to_embed = []
            new_metadata = []
            
            for article in articles:
                if article is None:  # articleì´ Noneì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                    continue
                # í…ìŠ¤íŠ¸ ì¡°í•© (ì œëª© + ìš”ì•½ + ë³¸ë¬¸)
                combined_text = f"{article.get('title', '')} {article.get('summary', '')} {article.get('full_text', '')}"
                processed_text = self.preprocess_text(combined_text)
                
                if len(processed_text) < 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                    continue
                    
                texts_to_embed.append(processed_text)
                
                metadata = {
                    'title': article.get('title', ''),
                    'summary': article.get('summary', ''),
                    'link': article.get('link', ''),
                    'category': article.get('category', ''),
                    'source': article.get('source', ''),
                    'published': article.get('published', ''),
                    'collected_at': article.get('collected_at', ''),
                    'text_length': len(processed_text)
                }
                new_metadata.append(metadata)
            
            if not texts_to_embed:
                logger.warning("ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ì„ë² ë”© ìƒì„±
            logger.info(f"{len(texts_to_embed)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.encoder.encode(texts_to_embed, show_progress_bar=True)
            
            # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            
            # FAISSì— ì¶”ê°€
            self.index.add(embeddings.astype('float32'))
            self.metadata.extend(new_metadata)
            
            # ChromaDBì—ë„ ì¶”ê°€
            ids = [f"doc_{len(self.metadata) - len(new_metadata) + i}" for i in range(len(texts_to_embed))]
            
            self.collection.add(
                documents=texts_to_embed,
                metadatas=new_metadata,
                ids=ids
            )
            
            logger.info(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— {len(texts_to_embed)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            
            # ë°ì´í„° ì €ì¥
            await self.save_data()
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    async def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            if not query.strip():
                return []
                
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.encoder.encode([query])
            query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)
            
            # FAISS ê²€ìƒ‰
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.metadata):
                    result = self.metadata[idx].copy()
                    result['similarity_score'] = float(score)
                    result['rank'] = i + 1
                    results.append(result)
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def get_relevant_context(self, query: str, max_context_length: int = 2000) -> str:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            search_results = await self.search(query, top_k=3)
            
            if not search_results:
                return ""
            
            context_parts = []
            current_length = 0
            
            for result in search_results:
                title = result.get('title', '')
                summary = result.get('summary', '')
                source = result.get('source', '')
                
                part = f"[{source}] {title}\n{summary}\n"
                
                if current_length + len(part) > max_context_length:
                    break
                    
                context_parts.append(part)
                current_length += len(part)
            
            context = "\n".join(context_parts)
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(context)}ì")
            
            return context
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""

class IntelligentResponseGenerator:
    """ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self, knowledge_base: VectorKnowledgeBase):
        self.knowledge_base = knowledge_base
        self.tech_keywords = {
            'ai': ['ì¸ê³µì§€ëŠ¥', 'AI', 'artificial intelligence', 'ë¨¸ì‹ ëŸ¬ë‹', 'machine learning', 'ë”¥ëŸ¬ë‹', 'deep learning'],
            'gpt': ['GPT', 'ChatGPT', 'OpenAI', 'LLM', 'ëŒ€í™”í˜• AI', 'ì–¸ì–´ëª¨ë¸'],
            'tech': ['ê¸°ìˆ ', 'í…Œí¬', 'technology', 'í˜ì‹ ', 'innovation', 'ìŠ¤íƒ€íŠ¸ì—…', 'startup'],
            'programming': ['í”„ë¡œê·¸ë˜ë°', 'programming', 'ê°œë°œ', 'development', 'ì½”ë”©', 'coding'],
            'cloud': ['í´ë¼ìš°ë“œ', 'cloud', 'AWS', 'Azure', 'GCP', 'ì„œë²„ë¦¬ìŠ¤'],
            'blockchain': ['ë¸”ë¡ì²´ì¸', 'blockchain', 'ì•”í˜¸í™”í', 'cryptocurrency', 'NFT', 'DeFi']
        }
    
    def detect_tech_category(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ê°ì§€"""
        query_lower = query.lower()
        detected_categories = []
        
        for category, keywords in self.tech_keywords.items():
            if any(keyword.lower() in query_lower for keyword in keywords):
                detected_categories.append(category)
        
        return detected_categories
    
    async def generate_enhanced_response(self, query: str, base_response: str) -> str:
        """ê¸°ì¡´ ì‘ë‹µì„ ìµœì‹  ì •ë³´ë¡œ ê°•í™”"""
        try:
            # ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ê°ì§€
            categories = self.detect_tech_category(query)
            
            if not categories:
                return base_response
            
            # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            context = await self.knowledge_base.get_relevant_context(query)
            
            if not context:
                return base_response
            
            # ì‘ë‹µ ê°•í™”
            enhanced_response = f"""{base_response}

ğŸ“° **ìµœì‹  ê´€ë ¨ ì •ë³´:**

{context}

ğŸ’¡ *ìœ„ ì •ë³´ëŠ” ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.*"""
            
            return enhanced_response
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ê°•í™” ì˜¤ë¥˜: {e}")
            return base_response
    
    async def get_tech_news_summary(self, category: str = None) -> str:
        """ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"""
        try:
            if category:
                query = f"{category} ìµœì‹  ë‰´ìŠ¤"
            else:
                query = "ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ AI GPT"
            
            search_results = await self.knowledge_base.search(query, top_k=5)
            
            if not search_results:
                return "ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            summary_parts = []
            for i, result in enumerate(search_results, 1):
                title = result.get('title', '')
                source = result.get('source', '')
                summary = result.get('summary', '')[:200] + "..." if len(result.get('summary', '')) > 200 else result.get('summary', '')
                
                part = f"{i}. **{title}**\n   ğŸ“° {source}\n   {summary}\n"
                summary_parts.append(part)
            
            final_summary = f"""ğŸ”¥ **ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½**

{chr(10).join(summary_parts)}

ğŸ“… *ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M')}*"""
            
            return final_summary
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

class KnowledgeUpdateScheduler:
    """ì§€ì‹ë² ì´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, collector: TechNewsCollector, knowledge_base: VectorKnowledgeBase):
        self.collector = collector
        self.knowledge_base = knowledge_base
        self.update_interval = 3600  # 1ì‹œê°„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
        self.last_update = None
        self.is_running = False
    
    async def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        self.is_running = True
        logger.info("ì§€ì‹ë² ì´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
        
        while self.is_running:
            try:
                await self.update_knowledge_base()
                await asyncio.sleep(self.update_interval)
                
            except Exception as e:
                logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„
    
    async def update_knowledge_base(self):
        """ì§€ì‹ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        try:
            logger.info("ì§€ì‹ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘")
            
            # collectorê°€ Noneì¸ ê²½ìš° ìƒˆë¡œ ìƒì„±
            if self.collector is None:
                self.collector = TechNewsCollector()
            
            # ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
            articles = await self.collector.collect_all_news()
            
            # articlesê°€ Noneì´ê±°ë‚˜ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
            if articles is None:
                articles = []
            
            if articles and len(articles) > 0:
                # ê¸°ì‚¬ ë‚´ìš© ìƒì„¸ ìˆ˜ì§‘ (ì¼ë¶€ë§Œ)
                enhanced_articles = []
                for article in articles[:20]:  # ìµœì‹  20ê°œë§Œ ìƒì„¸ ìˆ˜ì§‘
                    try:
                        if article is not None:  # articleì´ Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬
                            enhanced = await self.collector.enhance_article_content(article)
                            if enhanced is not None:
                                enhanced_articles.append(enhanced)
                            else:
                                # enhancedê°€ Noneì¸ ê²½ìš° ì›ë³¸ ê¸°ì‚¬ë¼ë„ ì¶”ê°€
                                enhanced_articles.append(article)
                    except Exception as e:
                        logger.debug(f"ê¸°ì‚¬ ìƒì„¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                        # ê¸°ë³¸ ê¸°ì‚¬ ì •ë³´ë¼ë„ ì¶”ê°€
                        if article is not None:
                            enhanced_articles.append(article)
                    await asyncio.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                
                if enhanced_articles:
                    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€
                    await self.knowledge_base.add_articles(enhanced_articles)
                    
                    self.last_update = datetime.now()
                    logger.info(f"ì§€ì‹ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(enhanced_articles)}ê°œ ê¸°ì‚¬ ì¶”ê°€")
                else:
                    logger.warning("ìƒì„¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.warning("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
        except Exception as e:
            logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        self.is_running = False
        logger.info("ì§€ì‹ë² ì´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë“¤
tech_collector = TechNewsCollector()
vector_kb = VectorKnowledgeBase()
response_generator = None
update_scheduler = None

async def initialize_knowledge_system():
    """ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global response_generator, update_scheduler
    
    try:
        logger.info("ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        
        # ë²¡í„° ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™”
        await vector_kb.initialize()
        
        # ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”
        response_generator = IntelligentResponseGenerator(vector_kb)
        
        # ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        update_scheduler = KnowledgeUpdateScheduler(tech_collector, vector_kb)
        
        # ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
        asyncio.create_task(update_scheduler.update_knowledge_base())
        
        # ìë™ ì—…ë°ì´íŠ¸ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
        asyncio.create_task(update_scheduler.start_scheduler())
        
        logger.info("ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        raise

async def get_enhanced_response(query: str, base_response: str) -> str:
    """ê°•í™”ëœ ì‘ë‹µ ìƒì„±"""
    if response_generator:
        return await response_generator.generate_enhanced_response(query, base_response)
    return base_response

async def get_tech_news_summary(category: str = None) -> str:
    """ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½"""
    if response_generator:
        return await response_generator.get_tech_news_summary(category)
    return "ì§€ì‹ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

async def search_knowledge_base(query: str, top_k: int = 5) -> List[Dict]:
    """ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰"""
    if vector_kb:
        return await vector_kb.search(query, top_k)
    return []