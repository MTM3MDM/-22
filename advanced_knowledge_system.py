# ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ - AI/GPT/ê¸°ìˆ  ë‰´ìŠ¤ ì „ë¬¸ ì²˜ë¦¬
# ë²¡í„° ê²€ìƒ‰, ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘, ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±

import asyncio
import aiohttp
import json
import logging
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import hashlib
import pickle
from pathlib import Path
from types import TracebackType
from typing import Coroutine

# ë²¡í„° ê²€ìƒ‰ ë° ì„ë² ë”©
try:
    import faiss  # type: ignore
except ImportError:
    faiss = None
import numpy as np
from sentence_transformers import SentenceTransformer
try:
    import chromadb
    from chromadb.config import Settings
except ImportError:
    chromadb = None


# ë‰´ìŠ¤ ë° ë°ì´í„° ìˆ˜ì§‘
import feedparser  # type: ignore
import requests
from bs4 import BeautifulSoup
try:
    import newspaper  # type: ignore
    from newspaper import Article  # type: ignore
except ImportError:
    newspaper = None

# í…ìŠ¤íŠ¸ ì²˜ë¦¬
import re
from collections import defaultdict, Counter
import tiktoken

logger = logging.getLogger(__name__)

class TechNewsCollector:
    """ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.news_sources = {
            'ai_news_2025': [
                'https://feeds.feedburner.com/oreilly/radar',
                'https://techcrunch.com/category/artificial-intelligence/feed/',
                'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
                'https://venturebeat.com/ai/feed/',
                'https://www.wired.com/feed/tag/ai/latest/rss',
                'https://spectrum.ieee.org/rss/blog/artificial-intelligence',
                'https://www.technologyreview.com/feed/',
                'https://www.artificialintelligence-news.com/feed/',
                'https://machinelearningmastery.com/feed/',
                'https://towardsdatascience.com/feed',
                'https://distill.pub/rss.xml',
            ],
            'gpt_llm_2025': [
                'https://openai.com/blog/rss.xml',
                'https://blog.google/technology/ai/rss/',
                'https://blogs.microsoft.com/ai/feed/',
                'https://www.anthropic.com/news/rss.xml',
                'https://huggingface.co/blog/feed.xml',
                'https://deepmind.google/discover/blog/rss.xml',
                'https://stability.ai/news/rss',
                'https://cohere.com/blog/rss.xml',
            ],
            'tech_startup_2025': [
                'https://feeds.feedburner.com/TechCrunch',
                'https://www.engadget.com/rss.xml',
                'https://arstechnica.com/feed/',
                'https://www.zdnet.com/news/rss.xml',
                'https://www.cnet.com/rss/news/',
                'https://techcrunch.com/category/startups/feed/',
                'https://www.producthunt.com/feed',
                'https://news.ycombinator.com/rss',
                'https://www.crunchbase.com/feed',
            ],
            'dev_programming_2025': [
                'https://github.blog/feed/',
                'https://stackoverflow.blog/feed/',
                'https://dev.to/feed',
                'https://www.freecodecamp.org/news/rss/',
                'https://css-tricks.com/feed/',
                'https://www.smashingmagazine.com/feed/',
                'https://blog.jetbrains.com/feed/',
            ],
            'korean_tech_2025': [
                'https://www.etnews.com/rss/S08.xml',
                'https://www.bloter.net/feed',
                'https://www.itworld.co.kr/rss/news.xml',
                'https://zdnet.co.kr/rss/news.xml',
                'https://www.boannews.com/rss/news.xml',
                'https://www.ddaily.co.kr/rss/S1N15.xml',
            ],
            'cloud_blockchain_2025': [
                'https://aws.amazon.com/blogs/aws/feed/',
                'https://cloud.google.com/blog/rss',
                'https://azure.microsoft.com/en-us/blog/feed/',
                'https://www.coindesk.com/arc/outboundfeeds/rss/',
                'https://cointelegraph.com/rss',
                'https://decrypt.co/feed',
            ]
        }
        
        self.session = None
        self.collected_articles: List[Dict[str, Any]] = []
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        )
        return self
        
    async def __aexit__(self, 
                      exc_type: Optional[type[BaseException]], 
                      exc_val: Optional[BaseException], 
                      exc_tb: Optional[TracebackType]):
        if self.session and not self.session.closed:
            await self.session.close()
    
    async def collect_from_rss(self, url: str, category: str) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        if not self.session or self.session.closed:
            logger.warning("Aiohttp ì„¸ì…˜ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RSS ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    
                    articles: List[Dict[str, Any]] = []
                    for entry in feed.entries[:10]:  # ìµœì‹  10ê°œë§Œ
                        article: Dict[str, Any] = {
                            'title': entry.get('title', ''),
                            'summary': entry.get('summary', ''),
                            'link': entry.get('link', ''),
                            'published': entry.get('published', ''),
                            'category': category,
                            'source': feed.feed.get('title', 'Unknown') if hasattr(feed, 'feed') else 'Unknown',
                            'collected_at': datetime.now().isoformat()
                        }
                        articles.append(article)
                    
                    logger.info(f"RSSì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘: {url}")
                    return articles
            return [] # response.statusê°€ 200ì´ ì•„ë‹Œ ê²½ìš°
                    
        except Exception as e:
            logger.error(f"RSS ìˆ˜ì§‘ ì˜¤ë¥˜ {url}: {e}")
            return []
    
    async def collect_all_news(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles: List[Dict[str, Any]] = []
        
        # ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±
        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        ) as session:
            self.session = session
            
            tasks: List[Coroutine[Any, Any, List[Dict[str, Any]]]] = []
            for category, urls in self.news_sources.items():
                for url in urls:
                    tasks.append(self.collect_from_rss(url, category))
            
            results = await asyncio.gather(*tasks)
            for result in results:
                if result:
                    all_articles.extend(result)

        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        seen_titles: set[str] = set()
        unique_articles: List[Dict[str, Any]] = []
        for article in all_articles:
            if article and article.get('title'):
                try:
                    title_hash = hashlib.md5(article['title'].encode()).hexdigest()
                    if title_hash not in seen_titles:
                        seen_titles.add(title_hash)
                        unique_articles.append(article)
                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    unique_articles.append(article)
        
        logger.info(f"ì´ {len(unique_articles)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles
    
    async def enhance_article_content(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ì‚¬ ë‚´ìš© ìƒì„¸ ìˆ˜ì§‘"""
        if not self.session or self.session.closed:
            logger.warning("Aiohttp ì„¸ì…˜ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return article

        try:
            link = article.get('link')
            if not link:
                return article
                
            async with self.session.get(link) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # newspaper3kë¡œ ë³¸ë¬¸ ì¶”ì¶œ
                    if newspaper and Article:
                        try:
                            news_article = Article(link)
                            news_article.set_html(html_content)
                            news_article.parse()
                            
                            article['full_text'] = news_article.text
                            article['authors'] = news_article.authors
                            article['keywords'] = news_article.keywords
                            
                        except Exception as e:
                            logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {link}: {e}")
                            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ì˜¤ë¥˜ {link}: {e}")
            
        return article

class VectorKnowledgeBase:
    """ë²¡í„° ê¸°ë°˜ ì§€ì‹ë² ì´ìŠ¤"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.encoder: Optional[SentenceTransformer] = None
        self.index: Optional["faiss.Index"] = None
        self.metadata: List[Dict[str, Any]] = []
        self.dimension = 384  # all-MiniLM-L6-v2ì˜ ì°¨ì›
        
        # ChromaDB ì„¤ì •
        self.chroma_client: Optional["chromadb.Client"] = None
        self.collection: Optional["chromadb.Collection"] = None
        
        self.data_dir = Path("knowledge_base")
        self.data_dir.mkdir(exist_ok=True)
        
    async def initialize(self):
        """ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if not faiss or not chromadb:
            logger.error("FAISS ë˜ëŠ” ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            # SentenceTransformer ëª¨ë¸ ë¡œë“œ
            self.encoder = SentenceTransformer(self.model_name)
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
            
            # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            if faiss:
                self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            
            # ChromaDB ì´ˆê¸°í™”
            if chromadb:
                self.chroma_client = chromadb.PersistentClient(
                    path=str(self.data_dir / "chroma_db")
                )
                
                try:
                    if self.chroma_client:
                        self.collection = self.chroma_client.get_collection("tech_knowledge")
                        logger.info("ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ")
                except Exception:
                    if self.chroma_client:
                        self.collection = self.chroma_client.create_collection(
                            name="tech_knowledge",
                            metadata={"description": "Tech news and AI knowledge base"}
                        )
                        logger.info("ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„±")
                
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            await self.load_existing_data()
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise
    
    async def load_existing_data(self):
        """ê¸°ì¡´ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        if not faiss:
            return
        try:
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            index_path = self.data_dir / "faiss_index.bin"
            if index_path.exists() and faiss:
                self.index = faiss.read_index(str(index_path))
                logger.info("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = self.data_dir / "metadata.pkl"
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    self.metadata = pickle.load(f)
                logger.info(f"ë©”íƒ€ë°ì´í„° {len(self.metadata)}ê°œ ë¡œë“œ")
                
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    async def save_data(self):
        """ë°ì´í„° ì €ì¥"""
        if not faiss or not self.index:
            return
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, str(self.data_dir / "faiss_index.bin"))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.data_dir / "metadata.pkl", 'wb') as f:
                pickle.dump(self.metadata, f)
                
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
            
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    async def add_articles(self, articles: List[Dict[str, Any]]):
        """ê¸°ì‚¬ë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€"""
        if not articles or not self.encoder or not self.index or not self.collection:
            logger.warning("ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¶”ê°€í•  ê¸°ì‚¬ê°€ ì—†ì–´ ê¸°ì‚¬ ì¶”ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
            
        try:
            texts_to_embed = []
            new_metadata = []
            
            for article in articles:
                if article is None:
                    continue
                # í…ìŠ¤íŠ¸ ì¡°í•© (ì œëª© + ìš”ì•½ + ë³¸ë¬¸)
                combined_text = f"{article.get('title', '')} {article.get('summary', '')} {article.get('full_text', '')}"
                processed_text = self.preprocess_text(combined_text)
                
                if len(processed_text) < 50:
                    continue
                    
                texts_to_embed.append(processed_text)
                
                metadata: Dict[str, Any] = {
                    'title': article.get('title', ''),
                    'summary': article.get('summary', ''),
                    'link': article.get('link', ''),
                    'category': article.get('category', ''),
                    'source': article.get('source', ''),
                    'published': article.get('published', ''),
                    'collected_at': article.get('collected_at', ''),
                    'text_length': len(processed_text)
                }
                new_metadata.append(metadata)
            
            if not texts_to_embed:
                logger.warning("ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # ì„ë² ë”© ìƒì„±
            logger.info(f"{len(texts_to_embed)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.encoder.encode(texts_to_embed, show_progress_bar=True)
            
            # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
            embeddings_np = np.array(embeddings, dtype=np.float32)
            faiss.normalize_L2(embeddings_np)
            
            # FAISSì— ì¶”ê°€
            self.index.add(embeddings_np)
            self.metadata.extend(new_metadata)
            
            # ChromaDBì—ë„ ì¶”ê°€
            ids = [f"doc_{len(self.metadata) - len(new_metadata) + i}" for i in range(len(texts_to_embed))]
            
            self.collection.add(
                documents=texts_to_embed,
                metadatas=new_metadata,
                ids=ids
            )
            
            logger.info(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— {len(texts_to_embed)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            
            # ë°ì´í„° ì €ì¥
            await self.save_data()
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì¶”ê°€ ì˜¤ë¥˜: {e}")
    
    async def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.encoder or not self.index:
            logger.warning("ì¸ì½”ë” ë˜ëŠ” ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        try:
            if not query.strip():
                return []
                
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.encoder.encode([query])
            query_embedding_np = np.array(query_embedding, dtype=np.float32)
            faiss.normalize_L2(query_embedding_np)
            
            # FAISS ê²€ìƒ‰
            scores, indices = self.index.search(query_embedding_np, top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.metadata):
                    result = self.metadata[idx].copy()
                    result['similarity_score'] = float(score)
                    result['rank'] = i + 1
                    results.append(result)
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    async def get_relevant_context(self, query: str, max_context_length: int = 2000) -> str:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            search_results = await self.search(query, top_k=3)
            
            if not search_results:
                return ""
            
            context_parts = []
            current_length = 0
            
            for result in search_results:
                title = result.get('title', '')
                summary = result.get('summary', '')
                source = result.get('source', '')
                
                part = f"[{source}] {title}\n{summary}\n"
                
                if current_length + len(part) > max_context_length:
                    break
                    
                context_parts.append(part)
                current_length += len(part)
            
            context = "\n".join(context_parts)
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(context)}ì")
            
            return context
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return ""

class IntelligentResponseGenerator:
    """ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±ê¸°"""
    
    def __init__(self, knowledge_base: VectorKnowledgeBase):
        self.knowledge_base = knowledge_base
        self.tech_keywords = {
            'ai': ['ì¸ê³µì§€ëŠ¥', 'AI', 'artificial intelligence', 'ë¨¸ì‹ ëŸ¬ë‹', 'machine learning', 'ë”¥ëŸ¬ë‹', 'deep learning'],
            'gpt': ['GPT', 'ChatGPT', 'OpenAI', 'LLM', 'ëŒ€í™”í˜• AI', 'ì–¸ì–´ëª¨ë¸'],
            'tech': ['ê¸°ìˆ ', 'í…Œí¬', 'technology', 'í˜ì‹ ', 'innovation', 'ìŠ¤íƒ€íŠ¸ì—…', 'startup'],
            'programming': ['í”„ë¡œê·¸ë˜ë°', 'programming', 'ê°œë°œ', 'development', 'ì½”ë”©', 'coding'],
            'cloud': ['í´ë¼ìš°ë“œ', 'cloud', 'AWS', 'Azure', 'GCP', 'ì„œë²„ë¦¬ìŠ¤'],
            'blockchain': ['ë¸”ë¡ì²´ì¸', 'blockchain', 'ì•”í˜¸í™”í', 'cryptocurrency', 'NFT', 'DeFi']
        }
    
    def detect_tech_category(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ê°ì§€"""
        query_lower = query.lower()
        detected_categories = []
        
        for category, keywords in self.tech_keywords.items():
            if any(keyword.lower() in query_lower for keyword in keywords):
                detected_categories.append(category)
        
        return detected_categories
    
    async def generate_enhanced_response(self, query: str, base_response: str) -> str:
        """ê¸°ì¡´ ì‘ë‹µì„ ìµœì‹  ì •ë³´ë¡œ ê°•í™”"""
        try:
            # ê¸°ìˆ  ì¹´í…Œê³ ë¦¬ ê°ì§€
            categories = self.detect_tech_category(query)
            
            if not categories:
                return base_response
            
            # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            context = await self.knowledge_base.get_relevant_context(query)
            
            if not context:
                return base_response
            
            # ì‘ë‹µ ê°•í™”
            enhanced_response = f"""{base_response}

ğŸ“° **ìµœì‹  ê´€ë ¨ ì •ë³´:**

{context}

ğŸ’¡ *ìœ„ ì •ë³´ëŠ” ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.*"""
            
            return enhanced_response
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ê°•í™” ì˜¤ë¥˜: {e}")
            return base_response
    
    async def get_tech_news_summary(self, category: str = None) -> str:
        """ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"""
        try:
            if category:
                query = f"{category} ìµœì‹  ë‰´ìŠ¤"
            else:
                query = "ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ AI GPT"
            
            search_results = await self.knowledge_base.search(query, top_k=5)
            
            if not search_results:
                return "ìš”ì•½í•  ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”."
            
            summary_parts = ["ğŸ“° **ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½**\n"]
            for result in search_results:
                title = result.get('title', 'ì œëª© ì—†ìŒ')
                link = result.get('link', '#')
                source = result.get('source', 'ì¶œì²˜ ë¶ˆëª…')
                summary_parts.append(f"â€¢ **[{source}] {title}**\n  [ë§í¬]({link})")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ê¸°ìˆ  ë‰´ìŠ¤ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”."

# --- ëª¨ë“ˆ API ---
# ì´ ì‹œìŠ¤í…œì˜ ê³µê°œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œëŠ” ì´ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

knowledge_system_instance: Optional[VectorKnowledgeBase] = None
response_generator_instance: Optional[IntelligentResponseGenerator] = None
is_initialized = False

async def initialize_knowledge_system(update_on_start: bool = True):
    """
    ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    - ì‘ë‹µ ìƒì„±ê¸°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
    - update_on_startê°€ Trueì´ë©´, ì‹œì‘ ì‹œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    global knowledge_system_instance, response_generator_instance, is_initialized
    
    if is_initialized:
        logger.info("ì§€ì‹ ì‹œìŠ¤í…œì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    try:
        logger.info("ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
        
        knowledge_system_instance = VectorKnowledgeBase()
        await knowledge_system_instance.initialize()
        
        response_generator_instance = IntelligentResponseGenerator(knowledge_system_instance)
        
        if update_on_start:
            logger.info("ì‹œì‘ ì‹œ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
            async with TechNewsCollector() as collector:
                articles = await collector.collect_all_news()
                if articles:
                    # ê¸°ì‚¬ ë‚´ìš© ìƒì„¸ ìˆ˜ì§‘ (ì„ íƒì , ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
                    # enhanced_articles = await asyncio.gather(
                    #     *[collector.enhance_article_content(art) for art in articles[:20]] # ì˜ˆ: 20ê°œë§Œ
                    # )
                    await knowledge_system_instance.add_articles(articles)
        
        is_initialized = True
        logger.info("ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ.")
        
    except Exception as e:
        logger.critical(f"ê³ ê¸‰ ì§€ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ, ê´€ë ¨ ì¸ìŠ¤í„´ìŠ¤ë¥¼ Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì˜¤ë¥˜ ì „íŒŒë¥¼ ë§‰ìŠµë‹ˆë‹¤.
        knowledge_system_instance = None
        response_generator_instance = None
        is_initialized = False

async def get_enhanced_response(query: str, base_response: str) -> str:
    """
    ê¸°ë³¸ ì‘ë‹µì— ìµœì‹  ê¸°ìˆ  ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ê°•í™”ëœ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not is_initialized or not response_generator_instance:
        logger.warning("ì§€ì‹ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì‘ë‹µ ê°•í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return base_response
    
    return await response_generator_instance.generate_enhanced_response(query, base_response)

async def get_tech_news_summary(category: str = None) -> str:
    """
    ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ ë˜ëŠ” ì „ì²´ ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ì— ëŒ€í•œ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not is_initialized or not response_generator_instance:
        logger.warning("ì§€ì‹ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ë‰´ìŠ¤ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return "ì§€ì‹ ì‹œìŠ¤í…œì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
    return await response_generator_instance.get_tech_news_summary(category)

async def search_knowledge_base(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    ë‚´ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    if not is_initialized or not knowledge_system_instance:
        logger.warning("ì§€ì‹ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì§€ì‹ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
        
    return await knowledge_system_instance.search(query, top_k)

# --- ìŠ¤ì¼€ì¤„ë§ëœ ì‘ì—… ---
async def scheduled_news_update():
    """
    ì •ê¸°ì ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    if not is_initialized:
        logger.info("ìŠ¤ì¼€ì¤„ëœ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸: ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    logger.info("ìŠ¤ì¼€ì¤„ëœ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘...")
    try:
        async with TechNewsCollector() as collector:
            articles = await collector.collect_all_news()
            if articles and knowledge_system_instance:
                await knowledge_system_instance.add_articles(articles)
        logger.info("ìŠ¤ì¼€ì¤„ëœ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëœ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)